{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Overview:\n",
        "In this assignment, you will apply dynamic programming techniques to solve\n",
        "a simple Markov Decision Process (MDP) problem. MDPs are commonly\n",
        "used to model sequential decision-making problems, and dynamic\n",
        "programming is a powerful approach to find the optimal policy and value\n",
        "function.\n",
        "\n",
        "Problem Statement:\n",
        "\n",
        "Imagine a simple grid world with the following characteristics:\n",
        "\n",
        "  • The grid world is represented as an 25x25 grid.\n",
        "\n",
        "  • Each cell in the grid can be in one of two states: empty,  or goal.\n",
        "  \n",
        "  • The goal state is always in the left most point i.e., the cell in the 1st row and 1st column.\n",
        "  \n",
        "  • Empty cells can be traversed by an agent.\n",
        "  \n",
        "  • The goal cell is where the agent should reach.\n",
        "  \n",
        "  • The agent can move in four directions: up, down, left, and right. It cannot move diagonally.\n",
        "  \n",
        "  • At each time step, the agent can choose to move in one of the available directions or stay in the same cell.\n",
        "  \n",
        "  • The agent receives a reward of -1 for each time step it takes to reach the goal cell.\n",
        "  \n",
        "  • The agent cannot leave the grid.\n",
        "\n",
        "Your Tasks:\n"
      ],
      "metadata": {
        "id": "qEBA3-i8mB9p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Define the MDP for the given grid world. You need to specify the state\n",
        "space, action space, transition probabilities, reward function, and\n",
        "discount factor.\n"
      ],
      "metadata": {
        "id": "GrP2MzWumO8-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A MDP is a tuple <S,A,P,R,y> where\n",
        "  \n",
        "  S is a finite set of states, in this case a 25 x 25 grid of 625 states\n",
        "\n",
        "  A is a finite set of actions, in this case four actions; up, left, down, right\n",
        "\n",
        "  P is a state transition probability matrix\n",
        "\n",
        "  R is a reward function\n",
        "\n",
        "  y is a discount factor"
      ],
      "metadata": {
        "id": "qMxU5gXDn_Hn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Implement the policy iteration using dynamic programming algorithms\n",
        "to solve the MDP. Here is a simple pseudocode as a refresher:\n",
        "\n",
        "  a. Start with an initial random policy.\n",
        "  \n",
        "  b. Implement policy evaluation to calculate the value function for the policy.\n",
        "  \n",
        "  c. Implement policy improvement to update the policy based on the value function.\n",
        "  \n",
        "  d. Repeat policy evaluation and improvement until the policy converges.\n"
      ],
      "metadata": {
        "id": "ffBUfv-GmR3h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# State space S\n",
        "GRID_SIZE = 25\n",
        "\n",
        "GOAL_STATE = (0, 0)\n",
        "\n",
        "#Discount factor y\n",
        "DISCOUNT_FACTOR = 0.8\n",
        "\n",
        "# Action space A\n",
        "ACTIONS = [(0, 1), (0, -1), (1, 0), (-1, 0), (0, 0)]  # Right, Left, Down, Up, Stay\n",
        "\n",
        "#Ensure the agent doesn't leave the board\n",
        "def is_valid_state(state):\n",
        "    return 0 <= state[0] < GRID_SIZE and 0 <= state[1] < GRID_SIZE\n",
        "\n",
        "#Function to see if we won the game\n",
        "def is_goal_state(state):\n",
        "    return state == GOAL_STATE\n",
        "\n",
        "#Function to move to a new state given an action\n",
        "def get_next_state(state, action):\n",
        "    next_state = (state[0] + action[0], state[1] + action[1])\n",
        "    return next_state if is_valid_state(next_state) else state\n",
        "\n",
        "#Start with a random policy into a numpy array\n",
        "def initialize_policy():\n",
        "    policy = np.random.choice(range(len(ACTIONS)), size=(GRID_SIZE, GRID_SIZE))\n",
        "    return policy\n",
        "\n",
        "#\n",
        "def policy_evaluation(policy, value_function):\n",
        "    theta = 1e-6\n",
        "    while True:\n",
        "        delta = 0\n",
        "        #For each state\n",
        "        for i in range(GRID_SIZE):\n",
        "            for j in range(GRID_SIZE):\n",
        "                state = (i, j)\n",
        "\n",
        "                #If its not the goal state update the state value\n",
        "                if not is_goal_state(state):\n",
        "                    action = ACTIONS[policy[i, j]]\n",
        "                    next_state = get_next_state(state, action)\n",
        "                    reward = -1\n",
        "                    new_value = reward + DISCOUNT_FACTOR * value_function[next_state[0], next_state[1]]\n",
        "                    delta = max(delta, abs(new_value - value_function[i, j]))\n",
        "                    value_function[i, j] = new_value\n",
        "        if delta < theta:\n",
        "            break\n",
        "\n",
        "#Update the policy based on given value function\n",
        "def policy_update(policy, value_function):\n",
        "    policy_stable = True\n",
        "    for i in range(GRID_SIZE):\n",
        "        for j in range(GRID_SIZE):\n",
        "            state = (i, j)\n",
        "            if not is_goal_state(state):\n",
        "                old_action = policy[i, j]\n",
        "                action_values = []\n",
        "                #for each state find the value of each action\n",
        "                for k, action in enumerate(ACTIONS):\n",
        "                    next_state = get_next_state(state, action)\n",
        "                    reward = -1\n",
        "                    action_values.append(reward + DISCOUNT_FACTOR * value_function[next_state[0], next_state[1]])\n",
        "                #select the action that maximizes reward\n",
        "                new_action = np.argmax(action_values)\n",
        "                policy[i, j] = new_action\n",
        "                #If nothing changes in the policy break\n",
        "                if old_action != new_action:\n",
        "                    policy_stable = False\n",
        "    return policy_stable\n",
        "\n",
        "#Combine the above two ie. evaluate improve evaluate etc until convergence\n",
        "def policy_iteration():\n",
        "    #Initialize a random first policy\n",
        "    policy = initialize_policy()\n",
        "    #Fill the grid with zeros\n",
        "    value_function = np.zeros((GRID_SIZE, GRID_SIZE))\n",
        "    while True:\n",
        "        policy_evaluation(policy, value_function)\n",
        "        policy_stable = policy_update(policy, value_function)\n",
        "        #if no changes are made ent the loop\n",
        "        if policy_stable:\n",
        "            break\n",
        "    return policy, value_function\n",
        "\n"
      ],
      "metadata": {
        "id": "qROV9YDPnRVg"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Visualize the results:\n",
        "\n",
        "  a. Show the optimal policy found using both policy iteration and value iteration.\n",
        "  \n",
        "  b. Calculate and display the optimal value function.\n"
      ],
      "metadata": {
        "id": "kNNfVwDOmUsE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    optimal_policy, optimal_value_function = policy_iteration()\n",
        "\n",
        "    def visualize_policy(policy):\n",
        "        action_symbols = ['Right', 'Left', 'Down', 'Up', 'S']\n",
        "        for i in range(GRID_SIZE):\n",
        "            row = ''\n",
        "            for j in range(GRID_SIZE):\n",
        "                if is_goal_state((i, j)):\n",
        "                    row += 'G '\n",
        "                else:\n",
        "                    row += action_symbols[policy[i, j]] + ' '\n",
        "            print(row)\n",
        "\n",
        "    def visualize_value_function(value_function):\n",
        "        for i in range(GRID_SIZE):\n",
        "            row = ''\n",
        "            for j in range(GRID_SIZE):\n",
        "                if is_goal_state((i, j)):\n",
        "                    row += 'G '\n",
        "                else:\n",
        "                    row += f'{value_function[i, j]:.2f} '\n",
        "            print(row)\n",
        "\n",
        "    print(\"Optimal Policy:\")\n",
        "    visualize_policy(optimal_policy)\n",
        "\n",
        "    print(\"\\nOptimal Value Function:\")\n",
        "    visualize_value_function(optimal_value_function)"
      ],
      "metadata": {
        "id": "ca_3Cy7WnRxy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "845ae2ef-593f-4190-94c1-24cf2c11d0ee"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal Policy:\n",
            "G Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left \n",
            "Up Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left \n",
            "Up Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left \n",
            "Up Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left \n",
            "Up Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left \n",
            "Up Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left \n",
            "Up Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left \n",
            "Up Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left \n",
            "Up Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left \n",
            "Up Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left \n",
            "Up Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left \n",
            "Up Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left \n",
            "Up Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left \n",
            "Up Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left \n",
            "Up Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left \n",
            "Up Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left \n",
            "Up Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left \n",
            "Up Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left \n",
            "Up Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left \n",
            "Up Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left \n",
            "Up Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left \n",
            "Up Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left \n",
            "Up Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left \n",
            "Up Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left \n",
            "Up Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left Left \n",
            "\n",
            "Optimal Value Function:\n",
            "G -1.00 -1.80 -2.44 -2.95 -3.36 -3.69 -3.95 -4.16 -4.33 -4.46 -4.57 -4.66 -4.73 -4.78 -4.82 -4.86 -4.89 -4.91 -4.93 -4.94 -4.95 -4.96 -4.97 -4.98 \n",
            "-1.00 -1.80 -2.44 -2.95 -3.36 -3.69 -3.95 -4.16 -4.33 -4.46 -4.57 -4.66 -4.73 -4.78 -4.82 -4.86 -4.89 -4.91 -4.93 -4.94 -4.95 -4.96 -4.97 -4.98 -4.98 \n",
            "-1.80 -2.44 -2.95 -3.36 -3.69 -3.95 -4.16 -4.33 -4.46 -4.57 -4.66 -4.73 -4.78 -4.82 -4.86 -4.89 -4.91 -4.93 -4.94 -4.95 -4.96 -4.97 -4.98 -4.98 -4.98 \n",
            "-2.44 -2.95 -3.36 -3.69 -3.95 -4.16 -4.33 -4.46 -4.57 -4.66 -4.73 -4.78 -4.82 -4.86 -4.89 -4.91 -4.93 -4.94 -4.95 -4.96 -4.97 -4.98 -4.98 -4.98 -4.99 \n",
            "-2.95 -3.36 -3.69 -3.95 -4.16 -4.33 -4.46 -4.57 -4.66 -4.73 -4.78 -4.82 -4.86 -4.89 -4.91 -4.93 -4.94 -4.95 -4.96 -4.97 -4.98 -4.98 -4.98 -4.99 -4.99 \n",
            "-3.36 -3.69 -3.95 -4.16 -4.33 -4.46 -4.57 -4.66 -4.73 -4.78 -4.82 -4.86 -4.89 -4.91 -4.93 -4.94 -4.95 -4.96 -4.97 -4.98 -4.98 -4.98 -4.99 -4.99 -4.99 \n",
            "-3.69 -3.95 -4.16 -4.33 -4.46 -4.57 -4.66 -4.73 -4.78 -4.82 -4.86 -4.89 -4.91 -4.93 -4.94 -4.95 -4.96 -4.97 -4.98 -4.98 -4.98 -4.99 -4.99 -4.99 -4.99 \n",
            "-3.95 -4.16 -4.33 -4.46 -4.57 -4.66 -4.73 -4.78 -4.82 -4.86 -4.89 -4.91 -4.93 -4.94 -4.95 -4.96 -4.97 -4.98 -4.98 -4.98 -4.99 -4.99 -4.99 -4.99 -5.00 \n",
            "-4.16 -4.33 -4.46 -4.57 -4.66 -4.73 -4.78 -4.82 -4.86 -4.89 -4.91 -4.93 -4.94 -4.95 -4.96 -4.97 -4.98 -4.98 -4.98 -4.99 -4.99 -4.99 -4.99 -5.00 -5.00 \n",
            "-4.33 -4.46 -4.57 -4.66 -4.73 -4.78 -4.82 -4.86 -4.89 -4.91 -4.93 -4.94 -4.95 -4.96 -4.97 -4.98 -4.98 -4.98 -4.99 -4.99 -4.99 -4.99 -5.00 -5.00 -5.00 \n",
            "-4.46 -4.57 -4.66 -4.73 -4.78 -4.82 -4.86 -4.89 -4.91 -4.93 -4.94 -4.95 -4.96 -4.97 -4.98 -4.98 -4.98 -4.99 -4.99 -4.99 -4.99 -5.00 -5.00 -5.00 -5.00 \n",
            "-4.57 -4.66 -4.73 -4.78 -4.82 -4.86 -4.89 -4.91 -4.93 -4.94 -4.95 -4.96 -4.97 -4.98 -4.98 -4.98 -4.99 -4.99 -4.99 -4.99 -5.00 -5.00 -5.00 -5.00 -5.00 \n",
            "-4.66 -4.73 -4.78 -4.82 -4.86 -4.89 -4.91 -4.93 -4.94 -4.95 -4.96 -4.97 -4.98 -4.98 -4.98 -4.99 -4.99 -4.99 -4.99 -5.00 -5.00 -5.00 -5.00 -5.00 -5.00 \n",
            "-4.73 -4.78 -4.82 -4.86 -4.89 -4.91 -4.93 -4.94 -4.95 -4.96 -4.97 -4.98 -4.98 -4.98 -4.99 -4.99 -4.99 -4.99 -5.00 -5.00 -5.00 -5.00 -5.00 -5.00 -5.00 \n",
            "-4.78 -4.82 -4.86 -4.89 -4.91 -4.93 -4.94 -4.95 -4.96 -4.97 -4.98 -4.98 -4.98 -4.99 -4.99 -4.99 -4.99 -5.00 -5.00 -5.00 -5.00 -5.00 -5.00 -5.00 -5.00 \n",
            "-4.82 -4.86 -4.89 -4.91 -4.93 -4.94 -4.95 -4.96 -4.97 -4.98 -4.98 -4.98 -4.99 -4.99 -4.99 -4.99 -5.00 -5.00 -5.00 -5.00 -5.00 -5.00 -5.00 -5.00 -5.00 \n",
            "-4.86 -4.89 -4.91 -4.93 -4.94 -4.95 -4.96 -4.97 -4.98 -4.98 -4.98 -4.99 -4.99 -4.99 -4.99 -5.00 -5.00 -5.00 -5.00 -5.00 -5.00 -5.00 -5.00 -5.00 -5.00 \n",
            "-4.89 -4.91 -4.93 -4.94 -4.95 -4.96 -4.97 -4.98 -4.98 -4.98 -4.99 -4.99 -4.99 -4.99 -5.00 -5.00 -5.00 -5.00 -5.00 -5.00 -5.00 -5.00 -5.00 -5.00 -5.00 \n",
            "-4.91 -4.93 -4.94 -4.95 -4.96 -4.97 -4.98 -4.98 -4.98 -4.99 -4.99 -4.99 -4.99 -5.00 -5.00 -5.00 -5.00 -5.00 -5.00 -5.00 -5.00 -5.00 -5.00 -5.00 -5.00 \n",
            "-4.93 -4.94 -4.95 -4.96 -4.97 -4.98 -4.98 -4.98 -4.99 -4.99 -4.99 -4.99 -5.00 -5.00 -5.00 -5.00 -5.00 -5.00 -5.00 -5.00 -5.00 -5.00 -5.00 -5.00 -5.00 \n",
            "-4.94 -4.95 -4.96 -4.97 -4.98 -4.98 -4.98 -4.99 -4.99 -4.99 -4.99 -5.00 -5.00 -5.00 -5.00 -5.00 -5.00 -5.00 -5.00 -5.00 -5.00 -5.00 -5.00 -5.00 -5.00 \n",
            "-4.95 -4.96 -4.97 -4.98 -4.98 -4.98 -4.99 -4.99 -4.99 -4.99 -5.00 -5.00 -5.00 -5.00 -5.00 -5.00 -5.00 -5.00 -5.00 -5.00 -5.00 -5.00 -5.00 -5.00 -5.00 \n",
            "-4.96 -4.97 -4.98 -4.98 -4.98 -4.99 -4.99 -4.99 -4.99 -5.00 -5.00 -5.00 -5.00 -5.00 -5.00 -5.00 -5.00 -5.00 -5.00 -5.00 -5.00 -5.00 -5.00 -5.00 -5.00 \n",
            "-4.97 -4.98 -4.98 -4.98 -4.99 -4.99 -4.99 -4.99 -5.00 -5.00 -5.00 -5.00 -5.00 -5.00 -5.00 -5.00 -5.00 -5.00 -5.00 -5.00 -5.00 -5.00 -5.00 -5.00 -5.00 \n",
            "-4.98 -4.98 -4.98 -4.99 -4.99 -4.99 -4.99 -5.00 -5.00 -5.00 -5.00 -5.00 -5.00 -5.00 -5.00 -5.00 -5.00 -5.00 -5.00 -5.00 -5.00 -5.00 -5.00 -5.00 -5.00 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Write a report describing your approach to the problem. It must contain\n",
        "the following:\n",
        "\n",
        "  a. The choice of data structure to implement the MDP and a justification\n",
        "\n",
        "  I used a simple numpy array randomly initialized with np.random.choice. It was just the most straightforward structure I had the most experience with.\n"
      ],
      "metadata": {
        "id": "pM6_kP75mbad"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to perform a greedy evaluation of a given policy\n",
        "def greedy_evaluation(policy, start_state, value_function):\n",
        "    state = start_state\n",
        "    steps = 0\n",
        "    while not is_goal_state(state):\n",
        "        action = ACTIONS[policy[state[0], state[1]]]\n",
        "        next_state = get_next_state(state, action)\n",
        "        state = next_state\n",
        "        steps += 1\n",
        "    return steps\n",
        "\n",
        "# Function to run experiments and report median steps\n",
        "def run_experiments(policy, value_function, num_trials=3):\n",
        "    start_states = [(1, 1), (10, 10), (20, 20)]  # Example start states\n",
        "\n",
        "    steps_list = []\n",
        "    for start_state in start_states:\n",
        "        steps_trial = []\n",
        "        for _ in range(num_trials):\n",
        "            steps = greedy_evaluation(policy, start_state, value_function)\n",
        "            steps_trial.append(steps)\n",
        "        steps_list.append(np.median(steps_trial))\n",
        "\n",
        "    return steps_list"
      ],
      "metadata": {
        "id": "t6AQlotn7iba"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# b. Performance of a random policy\n",
        "random_policy = initialize_policy()\n",
        "random_value_function = np.zeros((GRID_SIZE, GRID_SIZE))\n",
        "random_steps = run_experiments(random_policy, random_value_function)\n",
        "print(f\"Random Policy Performance: {random_steps}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "nqAjwmyzGVrU",
        "outputId": "c1fe20f3-9e58-4108-9f11-1656c1fe0377"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-de1b4a2affd5>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mrandom_policy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitialize_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mrandom_value_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGRID_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGRID_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mrandom_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_experiments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_policy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_value_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Random Policy Performance: {random_steps}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-6489385dd9c5>\u001b[0m in \u001b[0;36mrun_experiments\u001b[0;34m(policy, value_function, num_trials)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0msteps_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0msteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgreedy_evaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0msteps_trial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0msteps_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmedian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps_trial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-6489385dd9c5>\u001b[0m in \u001b[0;36mgreedy_evaluation\u001b[0;34m(policy, start_state, value_function)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_goal_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mACTIONS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mnext_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_next_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0msteps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-a045805bc9ab>\u001b[0m in \u001b[0;36mget_next_state\u001b[0;34m(state, action)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_next_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mnext_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnext_state\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_valid_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m#Start with a random policy into a numpy array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random performance will enter a permanent loop and will never complete because you know its random\n"
      ],
      "metadata": {
        "id": "rAjqBr95yUBA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "intermediate_policy, intermediate_value_function = policy_iteration()\n",
        "intermediate_steps = run_experiments(intermediate_policy, intermediate_value_function)\n",
        "print(f\"Intermediate Policy Performance: {intermediate_steps}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2kcYEJf-Acs",
        "outputId": "701f452c-b2e9-412e-aea1-59627e608bfb"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Intermediate Policy Performance: [2.0, 20.0, 40.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "  d. The performance of the optimal policy found using your approach. You can compute it by greedily evaluating it and reporting the median (out of three trials) number of steps it took to reach the goal state from 3 different start states. You should use the same start states as defined in the previous section for your experiments."
      ],
      "metadata": {
        "id": "jmKqsLKQVgNo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimal_policy, optimal_value_function = policy_iteration()\n",
        "optimal_steps = run_experiments(optimal_policy, optimal_value_function)\n",
        "print(f\"Optimal Policy Performance: {optimal_steps}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W7uhxt5I-AMH",
        "outputId": "3b79f97e-bb04-48e2-c63c-53dbc02c8047"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal Policy Performance: [2.0, 20.0, 40.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It would appear that since the intermediate and opitmal policys perform the same so the game is easily learned."
      ],
      "metadata": {
        "id": "gPCmjMUezgRQ"
      }
    }
  ]
}